{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5326cf45-9bdd-4a0e-8dc5-5591eb107532",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GEOG696C Spatiotemporal Data Analysis\n",
    "## Homework #4\n",
    "Alex Saunders | Last updated: 12 October 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae1429e-ed46-42c1-97bb-2c8d185a5abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt, colors\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.patheffects as pe\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import scipy\n",
    "import xarray\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs \n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828a8a40-33fc-4da6-8dad-abf47e5f41b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the root path\n",
    "# rootPath = Path('C:/Users/alexa/Documents/GitHub/spatiotemporal_data_analysis/00_hw')\n",
    "# rootPath = Path('C:/Users/alexsaunders/Documents/01_uoa/04_git/spatiotemporal_data_analysis/00_hw')\n",
    "# rootPath = Path('C:/Users/alexsaunders/Documents/01_uoa/04_git/spatiotemporal_data_analysis/00_hw')\n",
    "# dataPath = Path ('C:/Users/alexsaunders/Documents/01_uoa/01_study/2023/geog696c/data')\n",
    "rootPath = Path('C:/Users/alexa/Documents/GitHub/spatiotemporal_data_analysis/00_hw')\n",
    "dataPath = Path ('C:/Users/alexa/Documents/01_personal/07_phd/05_study/02_courses/2023/1/GEOG696/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844ad9e-df4c-462f-96c4-3b51dd5bdaa9",
   "metadata": {},
   "source": [
    "## 3. Calculate and map the eigen modes of variability of winter Pacific SSTs\n",
    "For the same domain as Deser and Blackon (1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e4e31-2ef4-4e8d-bc24-3353ecc98613",
   "metadata": {},
   "source": [
    "### Initial processing of the SST data as prep for SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f7a0c-a1fd-4dd0-99e1-abccd3319bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip uninstall netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6021f-840f-42a8-b4b3-82295d3ebab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb83a5f5-e11d-49e5-bc0e-4311a5e0edec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _netCDF4: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the SST data, HadISST Rayner et al 2003\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sstData \u001b[38;5;241m=\u001b[39m \u001b[43mxarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataPath\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHadISST_sst.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      3\u001b[0m sst \u001b[38;5;241m=\u001b[39m sstData\u001b[38;5;241m.\u001b[39msst\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Set missing values to na\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rio38\\lib\\site-packages\\xarray\\backends\\api.py:541\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    530\u001b[0m     decode_cf,\n\u001b[0;32m    531\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    537\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    538\u001b[0m )\n\u001b[0;32m    540\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 541\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    548\u001b[0m     backend_ds,\n\u001b[0;32m    549\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    558\u001b[0m )\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rio38\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:578\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    559\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    575\u001b[0m ):\n\u001b[0;32m    577\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 578\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rio38\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:349\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    348\u001b[0m ):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m    352\u001b[0m         filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rio38\\lib\\site-packages\\netCDF4\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# init for netCDF4. package\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Docstring comes from extension module _netCDF4.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_netCDF4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Need explicit imports for names beginning with underscores\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_netCDF4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;18m__doc__\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _netCDF4: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "# Load the SST data, HadISST Rayner et al 2003\n",
    "sstData = xarray.open_dataset(dataPath/'HadISST_sst.nc') \n",
    "sst = sstData.sst\n",
    "\n",
    "# Set missing values to na\n",
    "sst = sst.where(sst>=0, np.nan)\n",
    "\n",
    "# Transform the coord system from -180to180 longitude, to 0to360 degrees\n",
    "sst = sst.assign_coords(longitude=(sst.longitude % 360))\n",
    "# have to also sort the coordinates and associated data so they are in the correct order from 0 to 360\n",
    "sst = sst.sortby(sst.longitude)\n",
    "\n",
    "# Specify the desired area, and find corresponding start and end lat/lons that are present in the data\n",
    "lons=[120, 260]\n",
    "lats=[60, -20]\n",
    "sst = sst.sel(longitude=slice(lons[0], lons[1]), latitude=slice(lats[0], lats[1]))\n",
    "\n",
    "# Extract coordinates\n",
    "lat = sst.latitude\n",
    "lon = sst.longitude\n",
    "\n",
    "# We use the rolling mean technique, with window of 5 months, taking only the ones ending in March\n",
    "sstRolling = sst.rolling(time = 5).mean(skipna=True)\n",
    "\n",
    "# Select March to get the average over NDJFM\n",
    "endMo=3\n",
    "sstSeasonal = sstRolling[sstRolling.time.dt.month==endMo]\n",
    "\n",
    "# Drop the first which is an incomplete year\n",
    "sstSeasonal = sstSeasonal.drop_isel(time=[0])\n",
    "\n",
    "# Convert to np array for reshaping\n",
    "ntime, nlat, nlon = sstSeasonal.shape\n",
    "print(sstSeasonal.shape)\n",
    "sstMat = np.array(sstSeasonal).reshape(ntime, nlat*nlon, order='F')\n",
    "print(sstMat.shape)\n",
    "\n",
    "# Drop locations wtih missing values\n",
    "sstMatComplete = sstMat[:, ~np.isnan(sstMat).any(axis=0)]\n",
    "print(sstMatComplete.shape)\n",
    "\n",
    "# Get the mean and subtract it from the yearly seasonal average values, at ech location\n",
    "sstAnom = sstMatComplete - sstMatComplete.mean(axis=0)\n",
    "\n",
    "\n",
    "# Quick histogram to check the distribution of anomaly values centrs about zero\n",
    "plt.hist(np.mean(sstAnom, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455a28f-d0ee-45dd-a530-0d9449fe2c55",
   "metadata": {},
   "source": [
    "### Peform the SVD and get the EOFs (spatial) and PCs (temporal)\n",
    "Here we will perform SVD on the data anomalies themselves, rather than the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65411e23-710b-4ca2-8f1c-28c38231f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the years, and the location lat lons (excluding the missing value locations)\n",
    "years = range(sstSeasonal.time.dt.year.min().values, sstSeasonal.time.dt.year.max().values+1)\n",
    "sstMatOrig=np.array(sstSeasonal).reshape(len(sstSeasonal.time), len(sstSeasonal.latitude)*len(sstSeasonal.longitude), order='F')\n",
    "missingLocs = ~np.isnan(sstMatOrig).any(axis=0)\n",
    "locLats=[]\n",
    "locLons=[]\n",
    "allLats=[]\n",
    "allLons=[]\n",
    "i=0\n",
    "for lat in sstSeasonal.latitude.values:\n",
    "    for lon in sstSeasonal.longitude.values:\n",
    "        if missingLocs[i]==True:\n",
    "            locLats.append(lat)\n",
    "            locLons.append(lon)\n",
    "        allLats.append(lat)\n",
    "        allLons.append(lon)\n",
    "        i=i+1        \n",
    "latitudes = sst.latitude\n",
    "longitudes = sst.longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6cbe2-404e-4c1f-8e54-7edc1110a27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform SVD\n",
    "U, s, Vt = np.linalg.svd(sstAnom, full_matrices=False)\n",
    "print(U.shape, s.shape, Vt.shape)\n",
    "V=Vt.T\n",
    "\n",
    "# Create the eofs, with dimensions of the original lat lon grid\n",
    "eofs = np.zeros([nlat*nlon, ntime]) * np.nan # 2D\n",
    "eofs[missingLocs, :] = V\n",
    "eofs = eofs.reshape([nlat, nlon, ntime], order='F')\n",
    "print(eofs.shape)\n",
    "\n",
    "# Calculate the pcs\n",
    "pcs = sstAnom @ V\n",
    "print(pcs.shape)\n",
    "\n",
    "# Get the % of variance explained by the eigen modes from the real data\n",
    "eigenVals=(s**2)/(len(s)-1)\n",
    "expVar = (s / np.sum(s)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fa506-baa7-4263-8e5e-db07c3cc62bc",
   "metadata": {},
   "source": [
    "### Map the EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e1f96-c0fb-4e81-b5fc-bbf8bda5acf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "levels = np.linspace(-0.05, 0.05, 51)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12,12), constrained_layout=True, subplot_kw={'projection': ccrs.Miller(central_longitude=260)})\n",
    "\n",
    "fig.suptitle('Map of the empirical orthgonal function (EOF) signals for {0}-{1}'.format(min(years), max(years)), fontsize=18)\n",
    "\n",
    "# EOF1\n",
    "ax=axes[0]\n",
    "ax.set_title('EOF1', fontsize=16)\n",
    "ax.coastlines(color=\"black\") \n",
    "ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "ax.set_extent([lons[0], lons[1], lats[1], lats[0]], crs=ccrs.PlateCarree())\n",
    "gl1 = ax.gridlines(color='k', linestyle='--', draw_labels=[\"left\", \"bottom\"], x_inline=False,\n",
    "                  y_inline=False, ylocs=[-10, 0, 10, 20, 30, 40, 50, 60], xlocs=[120, 140, 160, 180, -160, -140, -120, -100])\n",
    "eof1Plot = ax.contourf(longitudes, latitudes, eofs[:,:,1], cmap='bwr', levels=levels,vmin=-0.05, vmax=0.05, transform=ccrs.PlateCarree()) \n",
    "cb1 = plt.colorbar(eof1Plot, orientation='vertical', ticks=np.linspace(-0.05, 0.05, 11))\n",
    "\n",
    "# EOF2\n",
    "ax=axes[1]\n",
    "ax.set_title('EOF2', fontsize=16)\n",
    "ax.coastlines(color=\"black\") \n",
    "ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "ax.set_extent([lons[0], lons[1], lats[1], lats[0]], crs=ccrs.PlateCarree())\n",
    "gl1 = ax.gridlines(color='k', linestyle='--', draw_labels=[\"left\", \"bottom\"], x_inline=False,\n",
    "                  y_inline=False, ylocs=[-10, 0, 10, 20, 30, 40, 50, 60], xlocs=[120, 140, 160, 180, -160, -140, -120, -100])\n",
    "eof2Plot = ax.contourf(longitudes, latitudes, eofs[:,:,0], cmap='bwr', levels=levels,vmin=-0.05, vmax=0.05, transform=ccrs.PlateCarree()) \n",
    "cb1 = plt.colorbar(eof2Plot, orientation='vertical', ticks=np.linspace(-0.05, 0.05, 11))\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398ed3e-4141-4e86-b474-dea6e4a9d173",
   "metadata": {},
   "source": [
    "### Plot the PC time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6791a-7d03-4ae3-a99b-314bacd686c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "ax.set_title('Time series of the empirical orthgonal function (EOF) signals', fontsize=18)\n",
    "\n",
    "ax.plot(years, pcs[:,0], label='EOF1')\n",
    "ax.plot(years, pcs[:,1], label='EOF2')\n",
    "\n",
    "ax.tick_params(which='both', labelsize=14)\n",
    "ax.set_xlabel('Year', fontsize=14)\n",
    "ax.set_ylabel('EOF Signal', fontsize=14)\n",
    "\n",
    "ax.legend(fontsize=16, ncol=2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fbcb6-2042-4308-b5c6-6d7510e7a23a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Perform a Rule N significance test using Gaussian random noise conditioned on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e6a47-70f6-4a59-9930-71dfa1a07369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sstAnom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b85668-70fa-4111-921b-b5b039109fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get mean and stdev of data values for conditioning the noise\n",
    "mu = sstMatComplete.mean(axis=0)\n",
    "sigma = sstMatComplete.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba499a-b541-469a-a8e0-92b62a684407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create empty list for saving eignevalues results\n",
    "eigenValsAll=[]\n",
    "for i in range(0, 100):\n",
    "    # Create random Gaussian matrix conditioned on the data (with mean and stdev), then subtract the mean\n",
    "    randomMat = np.random.normal(mu, sigma, sstMatComplete.shape) # sstAnom\n",
    "    randomMat = (randomMat - randomMat.mean(axis=0)) / randomMat.std(axis=0)\n",
    "    \n",
    "    # Get eigen vectors and eigen values\n",
    "    U, s, Vt = np.linalg.svd(randomMat, full_matrices=False)\n",
    "    V=Vt.T\n",
    "    eigenVals=(s**2)/(len(s)-1)\n",
    "    # Save the eigen values\n",
    "    eigenValsAll.append(eigenVals)\n",
    "    \n",
    "    # # calculate the covariance and then use svd to get the eigenvalues and eigenvectors\n",
    "    # covMat = np.cov(randomMat, rowvar=False, ddof=1) # get the covariance of the random matrix\n",
    "    # U, s, Vt = scipy.sparse.linalg.svds(covMat, k=153)\n",
    "    # # U, s, Vt = np.linalg.svd(covMat) # factor the covariance matrix into eigenvalues and eigenvectors\n",
    "    # eigenValsAll.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d61971-e1be-429e-bd53-703f81d08de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the 95%ile of the random data eigenvalues, for each mode\n",
    "pct=0.95\n",
    "modePcts=[]\n",
    "for mode in range(eigenValsAll[0].shape[0]):\n",
    "    eigenVals = [eigenVal[mode] for eigenVal in eigenValsAll]\n",
    "    modePct = np.percentile(eigenVals, pct)\n",
    "    modePcts.append(modePct)\n",
    "    \n",
    "# Convert the 95%ile eigenvalues for each mode into explained variance\n",
    "expVarRandom = (modePcts / np.sum(modePcts)) * 100\n",
    "\n",
    "# Determine which modes from our analysis 'pass' this significance test\n",
    "modesPassed = np.array(range(1, len(expVar)+1))[expVar>expVarRandom]\n",
    "print(modesPassed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9ff96-7295-456b-945d-b0d6f47aadbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Perform a Rule N significance test using red autocorrelated noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1cc651-5ab8-4054-a8ee-f011c744bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a mean phi value to use based on first order autocorrelation in the data\n",
    "acf1 = pd.DataFrame(sstAnom).apply(lambda x: x.autocorr())\n",
    "\n",
    "# Create empty list for saving simulated red noise eigenvalues\n",
    "eigenValsAll=[]\n",
    "\n",
    "# loop through the j iterations with an inner loop through the i rows of the simulated red noise matrix\n",
    "for j in range(0, 100):\n",
    "    \n",
    "    # Create an empty matrix to accept the red noise simulation\n",
    "    redMat = np.full(sstAnom.shape, np.nan) \n",
    "    \n",
    "    # mean value and the variance of the random distribution\n",
    "    c = sstAnom.mean(axis=0) # we have a zero-mean process\n",
    "    sigma_e = np.sqrt((1 - acf1 ** 2))  # sets the appropriate standard deviation for [0,1] data\n",
    "\n",
    "    # set the first value in the array as a random draw with the correct mean and variance\n",
    "    redMat[0,:] = np.array([c + np.random.normal(0, sigma_e, size=sstAnom.shape[1])])\n",
    "\n",
    "    # inner loop i that goes row-by-row to generate the red noise simulation\n",
    "    for i in range(1, sstAnom.shape[0]):\n",
    "        redMat[i,:] = np.array([c + (acf1 * redMat[i-1,:]) + np.random.normal(0, sigma_e, size=sstAnom.shape[1])])\n",
    "    \n",
    "    # Get eigen vectors and eigen values\n",
    "    U, s, Vt = np.linalg.svd(redMat, full_matrices=False)\n",
    "    V=Vt.T\n",
    "    eigenVals=(s**2)/(len(s)-1)\n",
    "    # Save the eigen values\n",
    "    eigenValsAll.append(eigenVals)\n",
    "    \n",
    "    # # calculate the covariance and then use svd to get the eigenvalues and eigenvectors\n",
    "    # covMat = np.cov(redMat, rowvar=False, ddof=1) # get the covariance of the random matrix\n",
    "    # U, s, Vt = np.linalg.svd(covMat, k=153) # factor the covariance matrix into eigenvalues and eigenvectors\n",
    "    # eigenValsAll.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d03827-c70e-45b0-ab2e-c68028eb88f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the 95%ile of the random data eigenvalues, for each mode\n",
    "pct=0.95\n",
    "modePcts=[]\n",
    "for mode in range(eigenValsAll[0].shape[0]):\n",
    "    eigenVals = [eigenVal[mode] for eigenVal in eigenValsAll]\n",
    "    modePct = np.percentile(eigenVals, pct)\n",
    "    modePcts.append(modePct)\n",
    "    \n",
    "# Convert the 95%ile eigenvalues for each mode into explained variance\n",
    "expVarRed = (modePcts / np.sum(modePcts)) * 100\n",
    "\n",
    "# Determine which modes from our analysis 'pass' this significance test\n",
    "modesPassed = np.array(range(1, len(expVar)+1))[expVar>expVarRed]\n",
    "print(modesPassed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9b575-9eeb-4710-85ce-f422b260bf48",
   "metadata": {},
   "source": [
    "### Plot the Rule N test with a scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fef02e-8caa-4029-a6c8-18a0f241521a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(range(1, len(expVar) + 1), expVar, label='Real data')#, use_line_collection=True)\n",
    "\n",
    "plt.setp(stemlines, color='k')\n",
    "plt.setp(markerline, marker='o', markersize=10, color='k', zorder=2)\n",
    "plt.setp(baseline, color='none', zorder=1)\n",
    "\n",
    "ax.plot(range(1, len(expVarRandom)+1), expVarRandom, color='blue', linestyle='--', label='Rule N Gaussian Noise')\n",
    "ax.plot(range(1, len(expVarRed)+1), expVarRed, color='red', linestyle='--', label='Rule N Red Noise')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Eigen Rank', fontsize=14)\n",
    "ax.set_ylabel('% of total variance explained', fontsize=14)\n",
    "ax.set_xticks(range(1, len(expVar) + 1))\n",
    "ax.set_xlim(0, 30)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e9fc0-682e-4a5c-9871-1faebc4afbd2",
   "metadata": {},
   "source": [
    "__How many modes of North Pacific winter SSTs are \"significant, meaningful, interpretable and/or differentiated from noise?\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd654a3-e663-4d71-934a-ce17108921a6",
   "metadata": {},
   "source": [
    "## 6. Perform a 'field correlation' analysis\n",
    "The correlation between a single time series and the time series at every point in the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddf86e-f70e-4402-900c-bcbbb45d5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and PCs in xarray for easy correlation\n",
    "pcsXr = xarray.DataArray(pcs,\n",
    "                        dims=(\"time\",\"mode\"),\n",
    "                        coords=[\n",
    "                            (\"time\", sstSeasonal.time.values),\n",
    "                            (\"mode\", range(1,len(years)+1)),\n",
    "                        ],)\n",
    "\n",
    "# sstAnomXr = xarray.DataArray(sstAnom,\n",
    "#                         dims=(\"year\",\"loc\"),\n",
    "#                         coords=[\n",
    "#                             (\"year\", years),\n",
    "#                             (\"loc\", range(1,sstAnom.shape[1]+1)),\n",
    "#                         ],)\n",
    "\n",
    "# For EOF 1 and EOF 2 time series, compute correlation with the original SST data\n",
    "fieldCorr = xarray.corr(sstSeasonal, pcsXr, dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1efd7-e9be-48ea-a20e-91509bb0b109",
   "metadata": {},
   "source": [
    "### Plot maps of field correlation for the first two modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab6162-ebfd-4755-9aac-c5180d0774e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "levels = np.linspace(-1, 1, 21)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12,12), constrained_layout=True, subplot_kw={'projection': ccrs.Miller(central_longitude=260)})\n",
    "\n",
    "fig.suptitle('Map of the EOF field correlation for {0}-{1}'.format(min(years), max(years)), fontsize=18)\n",
    "\n",
    "# EOF1\n",
    "ax=axes[0]\n",
    "ax.set_title('EOF1', fontsize=16)\n",
    "ax.coastlines(color=\"black\") \n",
    "ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "ax.set_extent([lons[0], lons[1], lats[1], lats[0]], crs=ccrs.PlateCarree())\n",
    "gl1 = ax.gridlines(color='k', linestyle='--', draw_labels=[\"left\", \"bottom\"], x_inline=False,\n",
    "                  y_inline=False, ylocs=[-10, 0, 10, 20, 30, 40, 50, 60], xlocs=[120, 140, 160, 180, -160, -140, -120, -100])\n",
    "eof1Plot = ax.contourf(longitudes, latitudes, fieldCorr.sel(mode=1).values, cmap='BrBG', levels=levels, vmin=-1, vmax=1, transform=ccrs.PlateCarree()) \n",
    "cb1 = plt.colorbar(eof1Plot, orientation='vertical', ticks=np.linspace(-1, 1, 11))\n",
    "\n",
    "# EOF2\n",
    "ax=axes[1]\n",
    "ax.set_title('EOF2', fontsize=16)\n",
    "ax.coastlines(color=\"black\") \n",
    "ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "ax.set_extent([lons[0], lons[1], lats[1], lats[0]], crs=ccrs.PlateCarree())\n",
    "gl1 = ax.gridlines(color='k', linestyle='--', draw_labels=[\"left\", \"bottom\"], x_inline=False,\n",
    "                  y_inline=False, ylocs=[-10, 0, 10, 20, 30, 40, 50, 60], xlocs=[120, 140, 160, 180, -160, -140, -120, -100])\n",
    "eof2Plot = ax.contourf(longitudes, latitudes, fieldCorr.sel(mode=2).values, cmap='BrBG', levels=levels, vmin=-1, vmax=1, transform=ccrs.PlateCarree()) \n",
    "cb1 = plt.colorbar(eof2Plot, orientation='vertical', ticks=np.linspace(-1, 1, 11))\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e3182-517a-4643-9766-bb06bec08dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "985392d8-102b-45ac-b9ff-766d5198ae14",
   "metadata": {},
   "source": [
    "__How do the maps compare with the results from Deser and Blackmon (1995)?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50518f3-dfa5-4fb2-96f5-5b73e6f53a03",
   "metadata": {},
   "source": [
    "## 7 Identify the years with the highest and lowest of the signal values, for EOF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bb0ae-ffea-40b4-9566-9b9f2a33a936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upper=pcsXr.sel(mode=1).quantile(0.90)\n",
    "lower=pcsXr.sel(mode=1).quantile(0.10)\n",
    "\n",
    "upperYears= [pd.Timestamp(item).year for item in pcsXr.sel(mode=1).time[pcsXr.sel(mode=1) >= upper].values]\n",
    "lowerYears= [pd.Timestamp(item).year for item in pcsXr.sel(mode=1).time[pcsXr.sel(mode=1) <= lower].values]\n",
    "\n",
    "print('Upper 10%ile: ', upperYears)\n",
    "print('Lower 10%ile: ', lowerYears)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87e429-9fea-4b0d-8f45-166f55d2faaa",
   "metadata": {},
   "source": [
    "## 8. Import the reanalysis 500mb height data and slice for the same time period as the SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006a265-9074-4237-8889-5d5d65b70afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(dataPath/'hgt.500mb.mon.mean.nc').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38795c6a-adbf-4eab-b80b-858425d9b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the SST data, HadISST Rayner et al 2003\n",
    "hgtData = xarray.open_dataset(dataPath/'hgt_500mb_mon_mean.nc') \n",
    "# hgt = sstData.sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3d32f-87e2-47c4-ba68-369101809c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
