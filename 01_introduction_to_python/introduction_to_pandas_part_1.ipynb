{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas\n",
    "\n",
    "This is the first part of a two part introduction to the way we'll use `Pandas`` in this class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` is another library for Python with tools for handling and operating on data.  It particularly excels at the type of data we might usually expect to find in flat text files, CSV files, and Excel spreadsheets.  It also has a powerful data structure called a `DataFrame` that streamlines the process of data handling and manipulation and time series analysis.  \n",
    "\n",
    "This notebook introduces you to some of the main functionality of `Pandas`.  The first thing we'll do is import the `Pandas` library, following standard Python conventions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # let's also get NumPy, since we often need it\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "\n",
    "One of the first things we'll use `Pandas` for in this class is to read data into our notebooks.  `Pandas` provides simple input functions for CSV, Excel, HTML, and [many others](https://pandas.pydata.org/docs/user_guide/io.html).  I've provides a very simple Excel file and CSV file in the directory where this notebook resides.  Note that if you need to see the contents of your current directory, you can make an external system call and `!ls`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls # using '!' makes a call out of the notebook into your system (terminal), where `ls` lists the files in your directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use [the `os` library](https://docs.python.org/3/library/os.html), which contains Python methods for accessing information from your operating system, including things like directory listings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(os.curdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's use `Pandas` and read in some data.  You can see the full description of [`pd.read_excel` here](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html), and [`pd.read_csv` here](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). Note that `pd.read_excel` might give you a non-fatal (informational) warning if you are using the more modern `.xlsx` suffix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel('mySimpleExcelFile.xls') \n",
    "text_data = pd.read_csv('myData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our Excel data, first.  If we ask Python what type of variable `excel_data` is, it tells us it is a DataFrame from `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(excel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at this DataFrame simply by typing the variable name on its own line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note a few things.  First, the print to screen is formatted nicely - for the DataFrame type, this is the default display behavior - nicer than most of what we've seen thus far in Python.  Similar to Excel, we see that the columns have labels.  Each row of numeric data also has an index value, starting from 0. \n",
    "\n",
    "You can use the method `.dtypes` to look at what data types each column is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, you might want to only look at the top or bottom of the DataFrame, which can be accomplished with `.head()` and `.tail()` methods.  Of course, for our very tiny example dataset, it won't make much of a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look just at the names of the columns, by calling the `.columns` method on the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the _index_ values -- we can think of these as row numbers, like we designate row numbers in Excel.  Of course, by default, here in Python these start with zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.index # printing this will show the starting value, the last value, and the step distance between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `.describe()` method on our DataFrame gives us some summary statistics by column - we can see the number of values, their mean, standard deviation, min, max, and some quantiles.  This can be useful as a check that we have brought in the data correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of Pandas is that it provides us lots of tools we can use to manipulate and manage our tabular data.  Like in NumPy, one thing we can do is transpose the rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort using our index values (the row numbers) using `.sort_index`, and specifying ascending or descending and which axis to sort along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.sort_index(axis=0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort by the values in the DataFrame, and specify which column to use (similar to how we might do in Excel) to do this sorting.  Here, for example, we sort by the values in the column \"score\" and the default is to sort from low to high.  Notice how the rest of the DataFrame is sorted accordingly, too.  You can see this most clearly in how the index values are no longer in numerical order: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.sort_values(by=\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are only a few of the ways you can sort or change the orientation or position of your DataFrame data.  We'll encounter others later in the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas [provides powerful ways to index and select sets and subsets of your data(https://pandas.pydata.org/docs/user_guide/indexing.html).  Here are a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can refer to columns by their names, and use this to operate on the columns, extract them out to other variables (Panda series, etc.).  For instance, we could pull the 'accuracy' column out of our DataFrame and assign it to it's own Pandas series called 'accuracy':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data[\"accuracy\"]\n",
    "accuracy = excel_data[\"accuracy\"] # if you look, this is now a Pandas Series of length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also refer to the rows by their index values - for instance, here we look at the first two rows (following Pythonic indexing logic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data[0:2] # get rows using Python counting logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.loc[]` method also allows us to refer to rows or columns of our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.loc[0:2,:] # get rows by their location (index, in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.loc[:,[\"position\",\"accuracy\"]] # get columns by location, using names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.loc[1:3,[\"accuracy\",\"score\"]] # get columns and rows by location, using names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant on `.loc[]` is `.iloc[]` which allows us to refer to columns and rows by their position or numerical location, e.g. 'the 1st column'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.iloc[:,1] # this selects column 1, which is the \"score\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or 'the first row':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.iloc[1,:] # this selects by the position of index value 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also slice and dice our DataFrame based on its internal characteristics.  For instance, the code below selects only the rows of the DataFrame where the value in the \"accuracy\" column is less than or equal to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data[excel_data[\"accuracy\"]<=3] # get the DataFrame only where accuracy is <= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also readily add data to our DataFrame - here I add another column of data, call it 'velocity' and specify the strings it contains.  Note how the DataFrame allows us to mix numeric and string data types in this way as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data[\"velocity\"] = [\"fast\", \"slow\", \"medium\", \"slow\", \"medium\"]\n",
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very nice feature of Pandas is that it provides us with a host of tools for dealing with missing data (NaN).  \n",
    "\n",
    "You can see more of the ways of working with missing data in Pandas here: https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "\n",
    "Let's add a missing data point into our current DataFrame, replacing the value of '39' in the data column (index 2, column 1) with NaN, using `np.nan` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.iloc[2,1] = np.nan # the value of 39 in the accuracy column\n",
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`.dropna()` allows us to exclude rows of data where NaNs are present](https://pandas.pydata.org/docs/user_guide/missing_data.html#dropping-axis-labels-with-missing-data-dropna).  In the example below, we tell Pandas to ignore any row where _any_ of the values are NaN (we could also specify `all` if we wanted to only exclude those records where all the data were missing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.dropna(how=\"any\") # row with missing \"accuracy\" entry is not displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily replace or [fill NaN values with other values using `.fillna()`](https://pandas.pydata.org/docs/user_guide/missing_data.html#filling-missing-values-fillna):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.fillna(9999) # fills missing data with 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a Boolean mask that tells us where data are missing using `.isna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(excel_data) # boolean mask where data are missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or where data AREN'T missing, using `.notna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notna(excel_data) # boolean mask where data are not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics in `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll look at some very basic statistical functionality in Pandas.  We'll return to this topic when we need to acquire additional tools to deal with our data.\n",
    "\n",
    "First, I'm going to drop the 'velocity' column of strings I added above, just for simplicity.  There are a few ways to remove a column from our DataFrame using `.drop()`.  If you also specify `inplace=True`, Pandas will remove the column in the DataFrame without the need to reassign the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.drop(columns=[\"velocity\"],inplace=True) # or, alternatively: excel_data = excel_data.drop(columns=[\"velocity\"]) \n",
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily take a sum down the columns using `.sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, [a range of other summarizing statistics are available](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html), including the `.mean()':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, we can also specify the dimension (or axis) along which we wish to calculate the statistics.  If we want to calculate for instance the mean of each _row_, we can specify `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.mean(axis=1) # mean across the columns using axis=1 (the mean of each row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we go through the class, we'll look at other specific examples of how to use Pandas for our needs.  If you'd like to learn more right now, there are some very useful tutorials here:\n",
    "\n",
    "https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with time series, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the date we'll use in the class will be time series.  Pandas has specific `datetime` functionality that can be quite useful for this purpose.   Let's see some examples of this.  I've provided a simple CSV file with daily water discharge data (in units of daily mean cubic feet per second discharge) for Boulder Creek, Colorado. We can use `pd.read_csv` as we did before to read these data into a Pandas DataFrame, but if we also specify `parse_dates` and indicate the name of the column in the CSV file which contains dates, Pandas will attempt to use this as the index for the DataFrame and consider it a datetime object. \n",
    "\n",
    "More details on this are available here: https://pandas.pydata.org/docs/user_guide/timeseries.html\n",
    "\n",
    "First, we import [datatime](https://docs.python.org/3/library/datetime.html), then we call Pandas `pd.read_csv` and specify that the column named 'date' in the CSV file is to be the datetime index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df = pd.read_csv('BoulderCreekDischarge.csv',parse_dates=['date'])\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the type of column 'date' is `datetime64`, indicating that Python recognizes this field as having time and date properties. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many cool things we might do with this.  This makes availabel as set of `DataFrame.dt.function())` methods that allow us to work with time and dates, which can be difficult otherwise (e.g. which months have how many days? which years have 365 or 366 days? etc.)\n",
    "\n",
    "For instance, we can ask Pandas with dates correspond to a month named Janaury and store the resulting Boolean in a Pandas series called my_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_index = df[\"date\"].dt.month_name()=='January'\n",
    "print(my_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then ask Pandas to only show us those values in the DataFrame that correspond to January, using that index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[my_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output above, you see that there are 1116 rows returned - these are the rows of the daily data in DataFrame `df` that are January!  How cool is that? \n",
    "\n",
    "There is a Pandas tutorial on handling DateTime here: https://pandas.pydata.org/docs/getting_started/intro_tutorials/09_timeseries.html#\n",
    "\n",
    "We could use this functionality, for instance, to `resample` out DataFrame from daily to monthly.  The various time aliases that Pandas uses are here: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases.  \n",
    "\n",
    "Below, let's resample from daily to monthly data.  We use `.set_index()` to tell Pandas to use 'date' as the index (not the row numbers).  Then, we tell 'resample()' to look at 1 month ('1M') and to calculate a mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to 1 monthly means from daily data:\n",
    "monthly_discharge = df.set_index('date').resample('1M').mean()\n",
    "monthly_discharge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we now have only 432 columns, which is the number of months in our data set.   Let's make a simple `matplotlib` plot to see our monthly mean discharge data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() # use this instead of `plt.figure` to get the axis (ax) object\n",
    "ax.plot(monthly_discharge, label='Monthly Discharge',color='blue',linewidth=1)  # plot the sine wav\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Mean Month Discharge (cubic feet per second )')\n",
    "plt.show()  # show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, huh?  What about if we wanted to see an annual average? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_discharge = df.set_index('date').resample('1A').mean()\n",
    "annual_discharge # annual values from 1987 to 2022!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # use this instead of `plt.figure` to get the axis (ax) object\n",
    "ax.plot(annual_discharge, label='Monthly Discharge',\n",
    "        color='blue', linewidth=1)  # plot the sine wav\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Mean Annual Discharge (cubic feet per second )')\n",
    "plt.show()  # show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our data, we'll be able to call on Pandas to get our data into Python, manipulate it according to column names or index values (including datatime), create temporal averages, and much more.  We'll continue to learn bits and pieces of Pandas functionality as we go along. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
